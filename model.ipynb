{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import gzip\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.linear(x, self.weight.to(x.dtype), None if self.bias is None else self.bias.to(x.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(self, x: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        return super()._conv_forward(x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 15976])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek conv1d\n",
    "x = torch.randn(1, 1, 16000)\n",
    "c = Conv1d(1, 16, 25)\n",
    "o = c(x)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    # channels dim needs to be even because, we need split it half and process with sin and cos\n",
    "    assert channels % 2 == 0\n",
    "    \n",
    "    # todo\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    \n",
    "    # todo\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "\n",
    "    # todo\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "\n",
    "    # todo\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinusoids(5, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "        \n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # todo\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "        \n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "\n",
    "        return self.out(wv), qk\n",
    "    \n",
    "    def qkv_attention(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk = q @ k\n",
    "        if mask is not None:\n",
    "            qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        qk = qk.float()\n",
    "\n",
    "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "        \n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (query): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (key): Linear(in_features=5, out_features=5, bias=False)\n",
       "  (value): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (out): Linear(in_features=5, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (MultiHeadAttention(n_state, n_head) if cross_attention else None)\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "        \n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state))\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "    \n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[Tensor] = None):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualAttentionBlock(\n",
       "  (attn): MultiHeadAttention(\n",
       "    (query): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (key): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (value): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (out): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (attn_ln): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "  )\n",
       "  (mlp_ln): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResidualAttentionBlock(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

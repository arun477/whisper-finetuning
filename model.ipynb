{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import gzip\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.linear(x, self.weight.to(x.dtype), None if self.bias is None else self.bias.to(x.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(self, x: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        return super()._conv_forward(x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 15976])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek conv1d\n",
    "x = torch.randn(1, 1, 16000)\n",
    "c = Conv1d(1, 16, 25)\n",
    "o = c(x)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    # channels dim needs to be even because, we need split it half and process with sin and cos\n",
    "    assert channels % 2 == 0\n",
    "    \n",
    "    # todo\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    \n",
    "    # todo\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "\n",
    "    # todo\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "\n",
    "    # todo\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinusoids(5, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "        \n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # todo\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "        \n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "\n",
    "        return self.out(wv), qk\n",
    "    \n",
    "    def qkv_attention(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk = q @ k\n",
    "        if mask is not None:\n",
    "            qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        qk = qk.float()\n",
    "\n",
    "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "        \n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (query): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (key): Linear(in_features=5, out_features=5, bias=False)\n",
       "  (value): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (out): Linear(in_features=5, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (MultiHeadAttention(n_state, n_head) if cross_attention else None)\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "        \n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state))\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "    \n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[Tensor] = None):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualAttentionBlock(\n",
       "  (attn): MultiHeadAttention(\n",
       "    (query): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (key): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (value): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (out): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (attn_ln): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "  )\n",
       "  (mlp_ln): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResidualAttentionBlock(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer('positional_embedding', sinusoids(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList([\n",
    "            ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        assert x.shape[1:] == self.positional_embedding.shape\n",
    "        x = (x + self.positional_embedding).to(x.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_post(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList([\n",
    "            ResidualAttentionBlock(n_state, n_head, cross_attention=True) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer('mask', mask, persistent=False)\n",
    "    \n",
    "    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        x = (self.token_embedding(x) + self.positional_embedding[offset : offset + x.shape[-1]])\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
    "        \n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_language_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWhisper\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dims: ModelDimensions):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 70\u001b[0m, in \u001b[0;36mWhisper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mapply(install_hooks)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache, hooks\n\u001b[0;32m---> 70\u001b[0m detect_language \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_language_function\u001b[49m\n\u001b[1;32m     71\u001b[0m transcribe \u001b[38;5;241m=\u001b[39m transcribe_function\n\u001b[1;32m     72\u001b[0m decode \u001b[38;5;241m=\u001b[39m decode_function\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detect_language_function' is not defined"
     ]
    }
   ],
   "source": [
    "class Whisper(nn.Module):\n",
    "    def __init__(self, dims: ModelDimensions):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.dims.n_mels,\n",
    "            self.dims.n_audio_ctx,\n",
    "            self.dims.n_audio_state,\n",
    "            self.dims.n_audio_head,\n",
    "            self.dims.n_audio_layer\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.dims.n_vocab,\n",
    "            self.dims.n_text_ctx,\n",
    "            self.dims.n_text_state,\n",
    "            self.dims.n_text_head,\n",
    "            self.dims.n_text_layer\n",
    "        )\n",
    "\n",
    "        all_heads = torch.zeros(self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool)\n",
    "        all_heads[self.dims.n_text_layer // 2 :] = True\n",
    "        self.register_buffer('alignment_heads', all_heads.to_sparse(), persistent=Flase)\n",
    "    \n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(gzip.decompress(base64.b85decode(dump)), dtype=bool).copy()\n",
    "        mask = torch.from_numpy(array).reshape(self.dims.n_text_layer, self.dims.n_text_head)\n",
    "        self.register_buffer('alignment_heads', mask.to_sparse(), persistent=False)\n",
    "    \n",
    "    def embed_audio(self, mel: Tensor):\n",
    "        return self.encoder(mel)\n",
    "    \n",
    "    def logits(self, tokens: Tensor, audio_features: Tensor):\n",
    "        return self.decoder(tokens, audio_features)\n",
    "    \n",
    "    def forward(self, mel: Tensor, tokens: Tensor) -> Dict[str, Tensor]:\n",
    "        return self.decoder(tokens, self.encoder(mel))\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.dims.n_vocab >= 51865\n",
    "    \n",
    "    @property\n",
    "    def num_languages(self):\n",
    "       return sefl.dims.n_vocab - 51765 - int(self.is_multilingual)\n",
    "    \n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.dims.n_next_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "        \n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "            \n",
    "        self.decoder.apply(install_hooks)\n",
    "\n",
    "        return cache, hooks\n",
    "    \n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, replace\n",
    "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DecodingOptions:\n",
    "    task: str = 'transcribe'\n",
    "    language: Optional[str] = None\n",
    "\n",
    "    temperature: float = 0.0\n",
    "    sample_len: Optional[int] = None\n",
    "    best_of: Optional[int] = None\n",
    "    beam_size: Optional[int] = None\n",
    "    patience: Optional[float] = None\n",
    "\n",
    "    length_penalty: Optional[float] = None\n",
    "    \n",
    "    prompt: Optional[Union[str, List[int]]] = None\n",
    "    prefix: Optional[Union[str, List[int]]] = None\n",
    "\n",
    "    suppress_tokens: Optional[Union[str, Iterable[int]]] = \"-1\"\n",
    "    suppress_blank: bool = True\n",
    "\n",
    "    without_timestamps: bool = False\n",
    "    max_initial_timestamp: Optional[float] = 1.0\n",
    "\n",
    "    fp16: bool = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DecodingResult:\n",
    "    audio_features: Tensor\n",
    "    language: str\n",
    "    language_probs: Optional[Dict[str, float]] = None\n",
    "    tokens: List[int] = field(default_factory=list)\n",
    "    text: str = \"\"\n",
    "    avg_logprob: float = np.nan\n",
    "    no_speech_prob: float = np.nan\n",
    "    temperature: float = np.nan\n",
    "    compression_ratio: float = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def rearrange_kv_cache(self, source_indices) -> None:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def cleanup_caching(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceRanker:\n",
    "    def rank(self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]) -> List[int]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDecoder:\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def update(self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor) -> Tuple[Tensor, bool]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def finalize(\n",
    "        self, tokens: Tensor, sum_logprobs: Tensor\n",
    "    ) -> Tuple[Sequence[Sequence[Tensor]], List[List[float]]]:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitFilter:\n",
    "    def apply(self, logits: Tensor, tokens: Tensor) -> None:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(\n",
    "    multilingual: bool,\n",
    "    *,\n",
    "    num_languages: int = 99,\n",
    "    language: Optional[str] = None,\n",
    "    task: Optional[str] = None,  # Literal[\"transcribe\", \"translate\", None]\n",
    ") :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodingTask:\n",
    "    inference: Inference\n",
    "    sequence_ranker: SequenceRanker\n",
    "    decoder: TokenDecoder\n",
    "    logit_filters: List[LogitFilter]\n",
    "\n",
    "    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n",
    "        self.model = model\n",
    "        \n",
    "        language = options.language or 'en'\n",
    "        tokenizer = get_tokenizer(\n",
    "            model.is_multilingual,\n",
    "            num_languages=model.num_languages,\n",
    "            language=language,\n",
    "            task=options.task\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        self.options: DecodingOptions = self._verify_options(options)\n",
    "\n",
    "        self.n_group: int = options.beam_size or options.best_of or 1\n",
    "        self.n_ctx: int = model.dims.n_text_ctx\n",
    "        self.sample_len: int = options.sample_len or model.dims.n_text_ctx // 2\n",
    "\n",
    "        self.sort_sequence: Tuple[int] = tokenizer.sort_sequence\n",
    "        if self.options.without_timestamps:\n",
    "            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps\n",
    "\n",
    "    \n",
    "    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:\n",
    "        if options.beam_size is not None and options.best_of is not None:\n",
    "            raise ValueError('beam_size and best_of cant be given together')\n",
    "        if options.temperature == 0:\n",
    "            if options.best_of is not None:\n",
    "                raise ValueError('best_of with greedy sampling (T=0) is not compatible')\n",
    "        if options.patience is not None and options.beam_size is None:\n",
    "            raise ValueError('patience requires beam_size to be given')\n",
    "        if options.length_penalty is not None and not (0 <= options.length_penalty <= 1):\n",
    "            raise ValueError('length_penalty (alpha) should be a value between 0 and 1')\n",
    "        return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodingTask:\n",
    "\n",
    "    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n",
    "\n",
    "        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence\n",
    "        if self.options.without_timestamps:\n",
    "            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps\n",
    "\n",
    "        self.initial_tokens: Tuple[int] = self._get_initial_tokens()\n",
    "        self.sample_begin: int = len(self.initial_tokens)\n",
    "        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)\n",
    "\n",
    "        # inference: implements the forward pass through the decoder, including kv caching\n",
    "        self.inference = PyTorchInference(model, len(self.initial_tokens))\n",
    "\n",
    "        # sequence ranker: implements how to rank a group of sampled sequences\n",
    "        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)\n",
    "\n",
    "        # decoder: implements how to select the next tokens, given the autoregressive distribution\n",
    "        if options.beam_size is not None:\n",
    "            self.decoder = BeamSearchDecoder(\n",
    "                options.beam_size, tokenizer.eot, self.inference, options.patience\n",
    "            )\n",
    "        else:\n",
    "            self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)\n",
    "\n",
    "        # logit filters: applies various rules to suppress or penalize certain tokens\n",
    "        self.logit_filters = []\n",
    "        if self.options.suppress_blank:\n",
    "            self.logit_filters.append(SuppressBlank(self.tokenizer, self.sample_begin))\n",
    "        if self.options.suppress_tokens:\n",
    "            self.logit_filters.append(SuppressTokens(self._get_suppress_tokens()))\n",
    "        if not options.without_timestamps:\n",
    "            precision = CHUNK_LENGTH / model.dims.n_audio_ctx  # usually 0.02 seconds\n",
    "            max_initial_timestamp_index = None\n",
    "            if options.max_initial_timestamp:\n",
    "                max_initial_timestamp_index = round(\n",
    "                    self.options.max_initial_timestamp / precision\n",
    "                )\n",
    "            self.logit_filters.append(\n",
    "                ApplyTimestampRules(\n",
    "                    tokenizer, self.sample_begin, max_initial_timestamp_index\n",
    "                )\n",
    "            )\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_initial_tokens(self) -> Tuple[int]:\n",
    "        tokens = list(self.sot_sequence)\n",
    "\n",
    "        if prefix := self.options.prefix:\n",
    "            prefix_tokens = (\n",
    "                self.tokenizer.encode(\" \" + prefix.strip())\n",
    "                if isinstance(prefix, str)\n",
    "                else prefix\n",
    "            )\n",
    "            if self.sample_len is not None:\n",
    "                max_prefix_len = self.n_ctx // 2 - self.sample_len\n",
    "                prefix_tokens = prefix_tokens[-max_prefix_len:]\n",
    "            tokens = tokens + prefix_tokens\n",
    "\n",
    "        if prompt := self.options.prompt:\n",
    "            prompt_tokens = (\n",
    "                self.tokenizer.encode(\" \" + prompt.strip())\n",
    "                if isinstance(prompt, str)\n",
    "                else prompt\n",
    "            )\n",
    "            tokens = (\n",
    "                [self.tokenizer.sot_prev]\n",
    "                + prompt_tokens[-(self.n_ctx // 2 - 1) :]\n",
    "                + tokens\n",
    "            )\n",
    "\n",
    "        return tuple(tokens)\n",
    "\n",
    "    def _get_suppress_tokens(self) -> Tuple[int]:\n",
    "        suppress_tokens = self.options.suppress_tokens\n",
    "\n",
    "        if isinstance(suppress_tokens, str):\n",
    "            suppress_tokens = [int(t) for t in suppress_tokens.split(\",\")]\n",
    "\n",
    "        if -1 in suppress_tokens:\n",
    "            suppress_tokens = [t for t in suppress_tokens if t >= 0]\n",
    "            suppress_tokens.extend(self.tokenizer.non_speech_tokens)\n",
    "        elif suppress_tokens is None or len(suppress_tokens) == 0:\n",
    "            suppress_tokens = []  # interpret empty string as an empty list\n",
    "        else:\n",
    "            assert isinstance(suppress_tokens, list), \"suppress_tokens must be a list\"\n",
    "\n",
    "        suppress_tokens.extend(\n",
    "            [\n",
    "                self.tokenizer.transcribe,\n",
    "                self.tokenizer.translate,\n",
    "                self.tokenizer.sot,\n",
    "                self.tokenizer.sot_prev,\n",
    "                self.tokenizer.sot_lm,\n",
    "            ]\n",
    "        )\n",
    "        if self.tokenizer.no_speech is not None:\n",
    "            # no-speech probability is collected separately\n",
    "            suppress_tokens.append(self.tokenizer.no_speech)\n",
    "\n",
    "        return tuple(sorted(set(suppress_tokens)))\n",
    "\n",
    "    def _get_audio_features(self, mel: Tensor):\n",
    "        if self.options.fp16:\n",
    "            mel = mel.half()\n",
    "\n",
    "        if mel.shape[-2:] == (\n",
    "            self.model.dims.n_audio_ctx,\n",
    "            self.model.dims.n_audio_state,\n",
    "        ):\n",
    "            # encoded audio features are given; skip audio encoding\n",
    "            audio_features = mel\n",
    "        else:\n",
    "            audio_features = self.model.encoder(mel)\n",
    "\n",
    "        if audio_features.dtype != (\n",
    "            torch.float16 if self.options.fp16 else torch.float32\n",
    "        ):\n",
    "            return TypeError(\n",
    "                f\"audio_features has an incorrect dtype: {audio_features.dtype}\"\n",
    "            )\n",
    "\n",
    "        return audio_features\n",
    "\n",
    "    def _detect_language(self, audio_features: Tensor, tokens: Tensor):\n",
    "        languages = [self.options.language] * audio_features.shape[0]\n",
    "        lang_probs = None\n",
    "\n",
    "        if self.options.language is None or self.options.task == \"lang_id\":\n",
    "            lang_tokens, lang_probs = self.model.detect_language(\n",
    "                audio_features, self.tokenizer\n",
    "            )\n",
    "            languages = [max(probs, key=probs.get) for probs in lang_probs]\n",
    "            if self.options.language is None:\n",
    "                tokens[:, self.sot_index + 1] = lang_tokens  # write language tokens\n",
    "\n",
    "        return languages, lang_probs\n",
    "\n",
    "    def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n",
    "        n_batch = tokens.shape[0]\n",
    "        sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n",
    "        no_speech_probs = [np.nan] * n_batch\n",
    "\n",
    "        try:\n",
    "            for i in range(self.sample_len):\n",
    "                logits = self.inference.logits(tokens, audio_features)\n",
    "\n",
    "                if (\n",
    "                    i == 0 and self.tokenizer.no_speech is not None\n",
    "                ):  # save no_speech_probs\n",
    "                    probs_at_sot = logits[:, self.sot_index].float().softmax(dim=-1)\n",
    "                    no_speech_probs = probs_at_sot[:, self.tokenizer.no_speech].tolist()\n",
    "\n",
    "                # now we need to consider the logits at the last token only\n",
    "                logits = logits[:, -1]\n",
    "\n",
    "                # apply the logit filters, e.g. for suppressing or applying penalty to\n",
    "                for logit_filter in self.logit_filters:\n",
    "                    logit_filter.apply(logits, tokens)\n",
    "\n",
    "                # expand the tokens tensor with the selected next tokens\n",
    "                tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\n",
    "\n",
    "                if completed or tokens.shape[-1] > self.n_ctx:\n",
    "                    break\n",
    "        finally:\n",
    "            self.inference.cleanup_caching()\n",
    "\n",
    "        return tokens, sum_logprobs, no_speech_probs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run(self, mel: Tensor) -> List[DecodingResult]:\n",
    "        self.decoder.reset()\n",
    "        tokenizer: Tokenizer = self.tokenizer\n",
    "        n_audio: int = mel.shape[0]\n",
    "\n",
    "        audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass\n",
    "        tokens: Tensor = torch.tensor([self.initial_tokens]).repeat(n_audio, 1)\n",
    "\n",
    "        # detect language if requested, overwriting the language token\n",
    "        languages, language_probs = self._detect_language(audio_features, tokens)\n",
    "        if self.options.task == \"lang_id\":\n",
    "            return [\n",
    "                DecodingResult(\n",
    "                    audio_features=features, language=language, language_probs=probs\n",
    "                )\n",
    "                for features, language, probs in zip(\n",
    "                    audio_features, languages, language_probs\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # repeat text tensors by the group size, for beam search or best-of-n sampling\n",
    "        tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n",
    "\n",
    "        # call the main sampling loop\n",
    "        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
    "\n",
    "        # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n",
    "        audio_features = audio_features[:: self.n_group]\n",
    "        no_speech_probs = no_speech_probs[:: self.n_group]\n",
    "        assert audio_features.shape[0] == len(no_speech_probs) == n_audio\n",
    "\n",
    "        tokens = tokens.reshape(n_audio, self.n_group, -1)\n",
    "        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n",
    "\n",
    "        # get the final candidates for each group, and slice between the first sampled token and EOT\n",
    "        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)\n",
    "        tokens: List[List[Tensor]] = [\n",
    "            [t[self.sample_begin : (t == tokenizer.eot).nonzero()[0, 0]] for t in s]\n",
    "            for s in tokens\n",
    "        ]\n",
    "\n",
    "        # select the top-ranked sample in each group\n",
    "        selected = self.sequence_ranker.rank(tokens, sum_logprobs)\n",
    "        tokens: List[List[int]] = [t[i].tolist() for i, t in zip(selected, tokens)]\n",
    "        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]\n",
    "\n",
    "        sum_logprobs: List[float] = [lp[i] for i, lp in zip(selected, sum_logprobs)]\n",
    "        avg_logprobs: List[float] = [\n",
    "            lp / (len(t) + 1) for t, lp in zip(tokens, sum_logprobs)\n",
    "        ]\n",
    "\n",
    "        fields = (\n",
    "            texts,\n",
    "            languages,\n",
    "            tokens,\n",
    "            audio_features,\n",
    "            avg_logprobs,\n",
    "            no_speech_probs,\n",
    "        )\n",
    "        if len(set(map(len, fields))) != 1:\n",
    "            raise RuntimeError(f\"inconsistent result lengths: {list(map(len, fields))}\")\n",
    "\n",
    "        return [\n",
    "            DecodingResult(\n",
    "                audio_features=features,\n",
    "                language=language,\n",
    "                tokens=tokens,\n",
    "                text=text,\n",
    "                avg_logprob=avg_logprob,\n",
    "                no_speech_prob=no_speech_prob,\n",
    "                temperature=self.options.temperature,\n",
    "                compression_ratio=compression_ratio(text),\n",
    "            )\n",
    "            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(\n",
    "                *fields\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def decode(model: 'Whisper', mel: Tensor, \n",
    "    options: DecodingOptions(), **kwargs) -> Union[DecodingResult, List[DecodingResult]]:\n",
    "    \n",
    "    if single := mel.ndim == 2:\n",
    "        mel = mel.unsqueeze(0)\n",
    "    \n",
    "    if kwargs:\n",
    "        options = replace(options, **kwargs)\n",
    "    \n",
    "    result = DecodingTask(model, options).run(mel)\n",
    "    \n",
    "    return result[0] if single else result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
